{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import h5py\n",
    "import nibabel as nib\n",
    "from pycocotools.coco import COCO\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model for multimodal embeddings...\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the CLIP model for multimodal embeddings\n",
    "print(\"Loading CLIP model for multimodal embeddings...\")\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_name)\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI mapping dictionaries for better visualization and analysis\n",
    "ROI_GROUPS = {\n",
    "    'prf-visualrois': ['V1v', 'V1d', 'V2v', 'V2d', 'V3v', 'V3d', 'hV4'],\n",
    "    'floc-bodies': ['EBA', 'FBA-1', 'FBA-2', 'mTL-bodies'],\n",
    "    'floc-faces': ['OFA', 'FFA-1', 'FFA-2', 'mTL-faces', 'aTL-faces'],\n",
    "    'floc-places': ['OPA', 'PPA', 'RSC'],\n",
    "    'floc-words': ['OWFA', 'VWFA-1', 'VWFA-2', 'mfs-words', 'mTL-words']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V1v', 'V1d', 'V2v', 'V2d', 'V3v', 'V3d', 'hV4', 'EBA', 'FBA-1', 'FBA-2', 'mTL-bodies', 'OFA', 'FFA-1', 'FFA-2', 'mTL-faces', 'aTL-faces', 'OPA', 'PPA', 'RSC', 'OWFA', 'VWFA-1', 'VWFA-2', 'mfs-words', 'mTL-words']\n"
     ]
    }
   ],
   "source": [
    "# Flatten ROI dictionary for easy access\n",
    "ALL_ROIS = []\n",
    "for rois in ROI_GROUPS.values():\n",
    "    ALL_ROIS.extend(rois)\n",
    "\n",
    "print(ALL_ROIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STIM_INFO_PATH = os.path.join(\"nsd_stim_info_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stimulus_info():\n",
    "    \"\"\"\n",
    "    Load and preprocess stimulus information from CSV\n",
    "    \"\"\"\n",
    "    stim_info = pd.read_csv(STIM_INFO_PATH, index_col=0)\n",
    "    important_columns = ['cocoId', 'cocoSplit', 'nsdId']\n",
    "    stim_info = stim_info[important_columns]\n",
    "    return stim_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cocoId</th>\n",
       "      <th>cocoSplit</th>\n",
       "      <th>nsdId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>532481</td>\n",
       "      <td>val2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245764</td>\n",
       "      <td>val2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>385029</td>\n",
       "      <td>val2017</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>311303</td>\n",
       "      <td>val2017</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>393226</td>\n",
       "      <td>val2017</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cocoId cocoSplit  nsdId\n",
       "0  532481   val2017      0\n",
       "1  245764   val2017      1\n",
       "2  385029   val2017      2\n",
       "3  311303   val2017      3\n",
       "4  393226   val2017      4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_nsd_to_coco=load_stimulus_info()\n",
    "mapping_nsd_to_coco.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coco_captions(nsdId, stim_info):\n",
    "    \"\"\"\n",
    "    Get COCO captions for a given NSD image ID\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get COCO ID and split\n",
    "        coco_id = stim_info[stim_info['nsdId'] == nsdId]['cocoId'].values[0]\n",
    "        coco_split = stim_info[stim_info['nsdId'] == nsdId]['cocoSplit'].values[0]\n",
    "        \n",
    "        # Load COCO annotation data\n",
    "        coco_annotation_file = os.path.join(f\"captions_{coco_split}.json\")\n",
    "        coco_data = COCO(coco_annotation_file)\n",
    "        coco_ann_ids = coco_data.getAnnIds(coco_id)\n",
    "        coco_annotations = coco_data.loadAnns(coco_ann_ids)\n",
    "        \n",
    "        # Extract captions\n",
    "        captions = [anno['caption'] for anno in coco_annotations]\n",
    "        return captions\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting captions for nsdId {nsdId}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_nsd_id(image_path):\n",
    "    \"\"\"\n",
    "    Extract nsd_id from the given image path.\n",
    "    \"\"\"\n",
    "    match = re.search(r'nsd-(\\d+)', image_path)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        raise ValueError(\"nsd_id not found in the image path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lh.all-vertices_fsaverage_space.npy    mapping_floc-words.npy\n",
      "lh.floc-bodies_challenge_space.npy     mapping_prf-visualrois.npy\n",
      "lh.floc-bodies_fsaverage_space.npy     mapping_streams.npy\n",
      "lh.floc-faces_challenge_space.npy      rh.all-vertices_fsaverage_space.npy\n",
      "lh.floc-faces_fsaverage_space.npy      rh.floc-bodies_challenge_space.npy\n",
      "lh.floc-places_challenge_space.npy     rh.floc-bodies_fsaverage_space.npy\n",
      "lh.floc-places_fsaverage_space.npy     rh.floc-faces_challenge_space.npy\n",
      "lh.floc-words_challenge_space.npy      rh.floc-faces_fsaverage_space.npy\n",
      "lh.floc-words_fsaverage_space.npy      rh.floc-places_challenge_space.npy\n",
      "lh.prf-visualrois_challenge_space.npy  rh.floc-places_fsaverage_space.npy\n",
      "lh.prf-visualrois_fsaverage_space.npy  rh.floc-words_challenge_space.npy\n",
      "lh.streams_challenge_space.npy         rh.floc-words_fsaverage_space.npy\n",
      "lh.streams_fsaverage_space.npy         rh.prf-visualrois_challenge_space.npy\n",
      "mapping_floc-bodies.npy                rh.prf-visualrois_fsaverage_space.npy\n",
      "mapping_floc-faces.npy                 rh.streams_challenge_space.npy\n",
      "mapping_floc-places.npy                rh.streams_fsaverage_space.npy\n"
     ]
    }
   ],
   "source": [
    "ls subj04/roi_masks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted nsd_id: 12\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A man and woman bathe in a shallow river.',\n",
       " 'A man and a woman in a river.',\n",
       " 'A man brushing his teeth in a river while a woman looks on.',\n",
       " 'A man and women in the river washing and eating.',\n",
       " 'An Indian man and woman in the water on the edge of a river.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "image_path = \"subj04/training_split/training_images/train-0001_nsd-00012.png\"\n",
    "nsd_id = extract_nsd_id(image_path)\n",
    "print(f\"Extracted nsd_id: {nsd_id}\")\n",
    "\n",
    "# Assuming mapping_nsd_to_coco is defined elsewhere in your code\n",
    "get_coco_captions(nsd_id, mapping_nsd_to_coco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
